import numpy as np
#import matplotlib.pyplot as plt
import csv
import sys
from keras.models import Sequential 
from keras.layers import Flatten, Dropout
from keras.layers import Dense 
from keras.models import load_model
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.pooling import MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
#from sklearn.metrics import confusion_matrix

# train data

file = open(sys.argv[1], 'r', encoding='big5')
train_data = []

for i in file:
    split = i.split(",")
    train_data.append(split)
file.close()

train_data = np.array(train_data)
train_data = np.delete(train_data, 0, 0)

train_x = []
for i in range(train_data.shape[0]):
    a = train_data[i][1].split( )
    for j in range(2304):
        a[j] = int(a[j])
    train_x.append(a)

train_x = np.array(train_x)

train_x = np.reshape(train_x, (train_x.shape[0], 48, 48, 1))

train_y = np.zeros((train_x.shape[0], 7))
for i in range(train_y.shape[0]):
    train_y[i][int(train_data[i][0])] = 1

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        zca_epsilon=1e-06,  # epsilon for ZCA whitening
        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)
        # randomly shift images horizontally (fraction of total width)
        width_shift_range=0.1,
        # randomly shift images vertically (fraction of total height)
        height_shift_range=0.1,
        shear_range=0.1,  # set range for random shear
        zoom_range=0.1,  # set range for random zoom
        channel_shift_range=0.1,  # set range for random channel shifts
        # set mode for filling points outside the input boundaries
        fill_mode='nearest',
        cval=0.,  # value used for fill_mode = "constant"
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False,  # randomly flip images
        # set rescaling factor (applied before any other transformation)
        rescale=None,
        # set function that will be applied on each input
        preprocessing_function=None,
        # image data format, either "channels_first" or "channels_last"
        data_format=None,
        # fraction of images reserved for validation (strictly between 0 and 1)
        validation_split=0.0)

    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
datagen.fit(train_x)

    # Fit the model on the batches generated by datagen.flow().



model = Sequential()  
model.add(Conv2D(32, kernel_size = (3, 3), strides=(1, 1), padding='same', input_shape = (48, 48, 1)))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))


model.add(Conv2D(32, kernel_size = (3, 3), strides=(1, 1), padding='same'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(Conv2D(32, kernel_size = (3, 3), strides=(1, 1), padding='same'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2), padding='same'))
model.add(Dropout(0.3))


model.add(Conv2D(64, kernel_size = (3, 3), strides=(1, 1), padding='same'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))


model.add(Conv2D(64, kernel_size = (3, 3), strides=(1, 1), padding='same'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(Conv2D(64, kernel_size = (3, 3), strides=(1, 1), padding='same'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(MaxPooling2D(pool_size=(2, 2), strides = (2, 2), padding='same'))
model.add(Dropout(0.3))

model.add(Conv2D(128, kernel_size = (2, 2), strides=(1, 1), padding='valid'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(Conv2D(128, kernel_size = (2, 2), strides=(1, 1), padding='valid'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(Conv2D(128, kernel_size = (2, 2), strides=(1, 1), padding='valid'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.3))

model.add(MaxPooling2D(pool_size=(2, 2), strides = (2, 2), padding='valid'))
model.add(Dropout(0.3))


model.add(Flatten())


model.add(Dense(units = 512))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(LeakyReLU(alpha=0.3))



model.add(Dense(units = 7, activation = 'softmax'))
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy','accuracy'])
history = model.fit_generator(datagen.flow(train_x, train_y, batch_size=100), steps_per_epoch=int(train_x.shape[0] / 100) , epochs=220, validation_data=(train_x[:6000,:,:,:], train_y[:6000,:]))
#history = model.fit(train_x, train_y, batch_size=100, epochs=50, validation_split=0.1)

'''
plt.plot(history.history["acc"])
plt.plot(history.history["val_acc"])
plt.title("Training process_CNN")
plt.xlabel("epoch")
plt.ylabel("acc")
plt.legend(['acc','val_acc'])
plt.savefig('Loss.png')
plt.show()
'''


model.save('my_model.h5')   
del model  # deletes the existing model
